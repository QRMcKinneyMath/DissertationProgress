\begin{thm}[Helly's Theorem]
    % \label{thm:helly} Old Label, keeping For Greps
    \label{thm:HellysTheorem}
    Let $(X,\norm{\cdot})$ be a \SeminormedSpace.
    Let $M>0$.
    Let $\{\alpha_i\}_{i=1}^n \subset \R$,
    Let $\{x_i^*\}_{i=1}^n \subset X^*$. 
    Then the following are equivalent. 
    \begin{enumerate}[label=(\roman*), ref={\ref{thm:HellysTheorem}~\roman*}]
        \item 
        \label{thm:HellysTheorem:1}
        For each $\epsilon > 0$
        , there is an $x_\epsilon \in X$ 
        such that $\norm{x_\epsilon}< M+\epsilon$ 
        and $\ip{x_\epsilon,x_i^*}=\alpha_i$ 
        for $1 \leq i \leq n$. 
        \item 
        \label{thm:HellysTheorem:2}
        For every $\epsilon > 0$
        , there is an $x_\epsilon \in X$ 
        such that $||x_\epsilon|| \leq M$ 
        and $\abs{\ip{x_\epsilon,x_i^*}-\alpha_i}< \epsilon$ 
        for $1 \leq i \leq n$. 
        \item 
        \label{thm:HellysTheorem:3}
        For each $\{\beta_i\}_{i=1}^n \subset \C$, 
        \begin{equation}
            \label{eq:HellysTheorem}
            \abs{\sum_{i=1}^n \beta_i \alpha_i} \leq M\norm{\sum_{i=1}^n \beta_i x_i^*}
        \end{equation}
    \end{enumerate}
    \begin{proof}[\ref{thm:HellysTheorem:1} $\implies$ \ref{thm:HellysTheorem:2}]
       Let $\epsilon > 0$. 
       Let $K=\sup\limits_{i=1}^n \norm{x_i^*}$. 
       Define $\epsilon_2=\frac{\epsilon}{2K}$.
       Then by \ref{thm:HellysTheorem:1}, there exists an $\tilde{x} \in X$ 
       with $\norm{\tilde{x}} < M+\epsilon_2$ 
       and $\ip{\tilde{x}, x_i^*} = \alpha_i$ for $1 \leq i \leq n$. 
       Define $x_0 = \frac{M}{M+\epsilon_2} \tilde{x}$. 
       Then $\norm{x_0} < M$. 
       Also, $\norm{\tilde{x}-x_0} \leq \epsilon_2$. 
       Furthermore, if $1 \leq i \leq n$, then we have
       \begin{align*}
       \abs{\ip{x_{0}, x_i^*} - \alpha_i} & \leq  \abs{\ip{x_0-\tilde{x}, x_i^*}}+ \abs{\ip{\tilde{x}, x_i^*}-\alpha_i}\\
       & = \abs{\ip{x_0-\tilde{x}, x_i^*}}
       & \leq K \epsilon_2\\
       & \leq \frac{\epsilon}{2} < \epsilon
       \end{align*}
       and so we are done
    \end{proof}
    \begin{proof}[\ref{thm:HellysTheorem:2} $\implies$ \ref{thm:HellysTheorem:3}]
        Let $\{\beta_i\}_{i=1}^n \subset \bbC$.
        Let $\epsilon > 0$. 
        Then by \ref{thm:HellysTheorem:2}, there exists 
        $x_{0} \in X$ with $\norm{x_0} \leq M$ and 
        $\abs{\ip{x_0, x_i^*} -\alpha_i} < \epsilon$ for $1 \leq i \leq n$. 
        Then
        \begin{align*}
        \abs{\sum\limits_{i=1}^n \beta_i \alpha_i} 
        & = \abs{\sum\limits_{i=1}^n \beta_i \pa{\alpha_i - \ip{x_0, x_i^*}+\ip{x_0, x_i^*}}}\\
        & \leq \abs{\sum\limits_{i=1}^n \beta_i \pa{\alpha_i - \ip{x_0, x_i^*}}}
        + \abs{\sum\limits_{i=1}^n  \beta_i \ip{x_0, x_i^*}}\\
        & < \epsilon \sum\limits_{i=1}^n \abs{\beta_i} 
        + \abs{ \ip{x_0, \sum\limits_{i=1}^n \beta_ix_i^*}}\\
        & \leq \epsilon \sum\limits_{i=1}^n \abs{\beta_i} 
        + M \norm{\sum\limits_{i=1}^n \beta_i x_i^*}
        \end{align*}
        Since $\epsilon > 0$ was arbitrary, we are done.
    \end{proof} 
    \begin{proof}[\ref{thm:HellysTheorem:3} $\implies$ \ref{thm:HellysTheorem:1}]
        If any $x_j^*=0$, then by selecting $\beta_i= \delta_{ij}$, 
        by \ref{thm:HellysTheorem:3}, we see that $\abs{\alpha_i} \leq M \norm{x_i^*} =0$. 
        Hence, if all $x_i^*=0$, 
        then all $\alpha_i = 0$, so 
        $x_\epsilon = 0$ works for any $\epsilon$. 
        Suppose at least some of the $\{x_i^*\}_{i=1}^n$ are nonzero.
        Then we can, by reordering, assume that for some integer $m$, 
        $1 \leq m \leq n$, we have 
        $\{x_i^*\}_{i=1}^m$ is
        \LinearlyIndependent,
        and also 
        $span \pa{\{x_i^*\}_{i=1}^m}= span \pa{\{x_i^*\}_{i=1}^n}$.
        %TODO 1: Macro For Starndard Basis
        Let $\{e_i\}_{i=1}^n$ be the standard basis for $\bbC^K$. 
        Define $S:X \to \bbC^m$ by 
        \begin{equation*}
        S(x) = \pa{\ip{x, x_1^*}, \ip{x, x_2^*}, \cdots, \ip{x, x_m^*}}
        \end{equation*}
        %TODO 2 Break the following Section into its own result about Dual Spaces
        I claim $S$ is \Surjective. 
        To see this suppose otherwise. 
        Then there exists $c \in \bbC^m$, represented $c=\sum_{i=1}^m$, such that 
        $c \perp Range(S)$. 
        Hence, for every $x \in X$, we would have 
        \begin{align*}
        0 & = \ip{\sum\limits_{i=1}^m \ip{x, x_i^*} e_i, \sum\limits_{i=1}^m c_i e_i}\\
        & = \sum_{i=1}^m c_i \ip{x, x_i^*}\\
        & = \ip{x, \sum\limits_{i=1}^m c_i x_i^*}
        \end{align*}
        This would of course imply $\{x_i\}_{i=1}^m$ was \LinearlyDependent, a contradiction. l
        Thus, $S$ is \Surjective. 
        %END of TODO 2
        Hence there exists  $\tilde{x} \in S^{-1}\pa{\alpha_1, \cdots, \alpha_m}$. 








        %%%%%%%%%%Old Proof
        \bf OLD PROOF \rm 

        If each $x_i^*=0$, then by    \ref{eq:helly}, each $\alpha_i=0$, so $x=0$ works for any $\epsilon$. 
        Otherwise, we without loss of generality reorder $\{x_i^*\}_{i=1}^n$ so that $\{x_i^*\}_{i=1}^m$ is linearly independent and $span\{x_i\}_{i=1}^m=span\{x_i\}_{i=1}^n$. 
        Define $S:X \to \C^{m}$ by $Sx=\bra{\ip{x,x_1^*},\cdots,\ip{x,x_m^*}}$, and recognize that linear independence of $\{x_i\}_{i=1}^n$ guarantees that S is surjective, so there exists 
        $\tilde{x} \in S^{-1} \{ \bra{\alpha_1,\cdots,\alpha_m}\}$ and furthermore
        \begin{equation}
            S^{-1}\{ \bra{\alpha_1,\cdots,\alpha_m}\}=\tilde{x}+ \bigcap_{i=1}^m kern(x_i^*) = \tilde{x}+ \bigcap_{i=1}^n kern(x_i^*):=\tilde{x}+K 
        \end{equation}
        If $m < j \leq n$, then $x_j^* \in span(x_i^*,\cdots,x_n^*)$ so  $x_j = \sum_{i=1}^n \gamma_i x_i^*$.
        Now we choose $\beta_i=\gamma_i$ for $1 \leq i \leq m$, $\beta_j=-1$, and other $\beta_k's=0$. 
        \begin{align} 
            \abs{\ip{\tilde{x},x_j^*}-\alpha_j}&= \abs{\ip{\tilde{x},\sum_{i=1}^m \beta_i x_i^*}-\alpha_j} = \abs{ \pa{\sum_{i=1}^m \beta_i \ip{\tilde{x},x_i^*}}+(-1)\alpha_j}\\
            &=\abs{\sum_{i=1}^n \beta_i \alpha_i} \\
            &\leq M \norm{\sum_{i=1}^n \beta_i x_i^*} \\
            & =M\norm{\sum_{i=1}^m \gamma_i x_i^* + (-1) x_j^*}
        \end{align}
        Hence $\ip{x,x_i^*} = \alpha_i$ for $1 \leq i \leq n$ and any $x \in \tilde{x}+K$. 
        By the Hahn Banach Theorem applied to the semi norm on X induced by the seminorm on the quotient space $X/K$, there is an $x^* \in X^*$ such that $\sup\limits_{\norm{x} \neq 0} \frac{\abs{\ip{x,x^*}}}{\norm{x}} =1$, $\ip{\tilde{x},x^*} = d(\tilde{x},K)$, and $K \subset ker(x^*)$. Since $K \subset ker(x^*)$, $x^* \in span \{ x_i^*\}_{i=1}^n$, say $x^* = \sum_{i=1}^n \mu_i x_i^*$. 
        Hence, 
        \begin{equation}
            \begin{split}
                d(\tilde{x},K) = \ip{\tilde{x},x^*} & = \sum_{i=1}^n \mu_i \ip{\tilde{x},x_i^*}\\
                & = \sum_{i=1}^n \mu_i \alpha_i\\
                & \leq M\norm{\sum_{i=1}^n \mu_i x_i^*}=M 
            \end{split}
        \end{equation}
        Thus, given $\epsilon > 0$, we find an $z_\epsilon \in K$ such that $||\tilde{x}-z_\epsilon|| < M+\epsilon$ and let $x_\epsilon=\tilde{x}-z_\epsilon$. 
        
    \end{proof}
    
\end{thm}


