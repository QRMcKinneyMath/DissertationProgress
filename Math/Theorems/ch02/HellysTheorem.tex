\begin{thm}[Helly's Theorem]
    % \label{thm:helly} Old Label, keeping For Greps
    \label{thm:HellysTheorem}
    Let $(X,\norm{\cdot})$ be a \SeminormedSpace.
    Let $M>0$.
    Let $\{\alpha_i\}_{i=1}^n \subset \R$,
    Let $\{x_i^*\}_{i=1}^n \subset X^*$. 
    Then the following are equivalent. 
    \begin{enumerate}[label=(\roman*), ref={\ref{thm:HellysTheorem}~\roman*}]
        \item 
        \label{thm:HellysTheorem:1}
        For each $\epsilon > 0$
        , there is an $x_\epsilon \in X$ 
        such that $\norm{x_\epsilon}< M+\epsilon$ 
        and $\ip{x_\epsilon,x_i^*}=\alpha_i$ 
        for $1 \leq i \leq n$. 
        \item 
        \label{thm:HellysTheorem:2}
        For every $\epsilon > 0$
        , there is an $x_\epsilon \in X$ 
        such that $||x_\epsilon|| \leq M$ 
        and $\abs{\ip{x_\epsilon,x_i^*}-\alpha_i}< \epsilon$ 
        for $1 \leq i \leq n$. 
        \item 
        \label{thm:HellysTheorem:3}
        For each $\{\beta_i\}_{i=1}^n \subset \C$, 
        \begin{equation}
            \label{eq:HellysTheorem}
            \abs{\sum_{i=1}^n \beta_i \alpha_i} \leq M\norm{\sum_{i=1}^n \beta_i x_i^*}
        \end{equation}
    \end{enumerate}
    \begin{proof}[\ref{thm:HellysTheorem:1} $\implies$ \ref{thm:HellysTheorem:2}]
       Let $\epsilon > 0$. 
       Let $K=\sup\limits_{i=1}^n \norm{x_i^*}$. 
       Define $\epsilon_2=\frac{\epsilon}{2K}$.
       Then by \ref{thm:HellysTheorem:1}, there exists an $\tilde{x} \in X$ 
       with $\norm{\tilde{x}} < M+\epsilon_2$ 
       and $\ip{\tilde{x}, x_i^*} = \alpha_i$ for $1 \leq i \leq n$. 
       Define $x_0 = \frac{M}{M+\epsilon_2} \tilde{x}$. 
       Then $\norm{x_0} < M$. 
       Also, $\norm{\tilde{x}-x_0} \leq \epsilon_2$. 
       Furthermore, if $1 \leq i \leq n$, then we have
       \begin{align*}
       \abs{\ip{x_{0}, x_i^*} - \alpha_i} & \leq  \abs{\ip{x_0-\tilde{x}, x_i^*}}+ \abs{\ip{\tilde{x}, x_i^*}-\alpha_i}\\
       & = \abs{\ip{x_0-\tilde{x}, x_i^*}}
       & \leq K \epsilon_2\\
       & \leq \frac{\epsilon}{2} < \epsilon
       \end{align*}
       and so we are done
    \end{proof}
    \begin{proof}[\ref{thm:HellysTheorem:2} $\implies$ \ref{thm:HellysTheorem:3}]
        Let $\{\beta_i\}_{i=1}^n \subset \bbC$.
        Let $\epsilon > 0$. 
        Then by \ref{thm:HellysTheorem:2}, there exists 
        $x_{0} \in X$ with $\norm{x_0} \leq M$ and 
        $\abs{\ip{x_0, x_i^*} -\alpha_i} < \epsilon$ for $1 \leq i \leq n$. 
        Then
        \begin{align*}
        \abs{\sum\limits_{i=1}^n \beta_i \alpha_i} 
        & = \abs{\sum\limits_{i=1}^n \beta_i \pa{\alpha_i - \ip{x_0, x_i^*}+\ip{x_0, x_i^*}}}\\
        & \leq \abs{\sum\limits_{i=1}^n \beta_i \pa{\alpha_i - \ip{x_0, x_i^*}}}
        + \abs{\sum\limits_{i=1}^n  \beta_i \ip{x_0, x_i^*}}\\
        & < \epsilon \sum\limits_{i=1}^n \abs{\beta_i} 
        + \abs{ \ip{x_0, \sum\limits_{i=1}^n \beta_ix_i^*}}\\
        & \leq \epsilon \sum\limits_{i=1}^n \abs{\beta_i} 
        + M \norm{\sum\limits_{i=1}^n \beta_i x_i^*}
        \end{align*}
        Since $\epsilon > 0$ was arbitrary, we are done.
    \end{proof} 
    \begin{proof}[\ref{thm:HellysTheorem:3} $\implies$ \ref{thm:HellysTheorem:1}]
        If any $x_j^*=0$, then by selecting $\beta_i= \delta_{ij}$, 
        by \ref{thm:HellysTheorem:3}, we see that $\abs{\alpha_i} \leq M \norm{x_i^*} =0$. 
        Hence, if all $x_i^*=0$, 
        then all $\alpha_i = 0$, so 
        $x_\epsilon = 0$ works for any $\epsilon$. 
        Suppose at least some of the $\{x_i^*\}_{i=1}^n$ are nonzero.
        Then we can, by reordering, assume that for some integer $m$, 
        $1 \leq m \leq n$, we have 
        $\{x_i^*\}_{i=1}^m$ is
        \LinearlyIndependent,
        and also 
        \begin{equation}
        \label{Eq:HellysTheorem:1}
        span \pa{\{x_i^*\}_{i=1}^m}= span \pa{\{x_i^*\}_{i=1}^n}
        \end{equation}
        %TODO 1: Macro For Starndard Basis
        Let $\{e_i\}_{i=1}^n$ be the standard basis for $\bbC^K$. 
        Define $S:X \to \bbC^m$ by 
        \begin{equation*}
        S(x) = \pa{\ip{x, x_1^*}, \ip{x, x_2^*}, \cdots, \ip{x, x_m^*}}
        \end{equation*}
        By \ref{prop:LinearlyIndependentLinearFunctionals}, 
        $S$ is \Surjective. 
        Hence there exists  $\tilde{x} \in S^{-1}\pa{\alpha_1, \cdots, \alpha_m}$. 
        Then, for $1 \leq i \leq m$, $\ip{\tilde{x}, x_i^*} = \alpha_i$. 
        Define $K=\bigcap\limits_{i=1}^{m}Kern(x_i^*)$. 
        Then $S^{-1}\pa{\alpha_1, \cdots, \alpha_m} = \tilde{X}+K$. 
        Let $m<j \leq n$, $j \in \mathbb{Z}$. Then, by 
        \ref{Eq:HellysTheorem:1}, 
        There exists $\{\gamma_i\}_{i=1}^m \subset \bbC^m$ such that 
        \begin{equation*}
        x_j^*= \sum_{i=1}^m \gamma_i x_i^*
        \end{equation*}
        Now define $\{\beta_i\}_{i=1}^n $ by 
        \begin{equation}
        \beta_i = \begin{dcases}
        \gamma_i & 1 \leq i \leq m \\
        0 & m < i \wedge i \neq j \\
        -1 & m < i \wedge i = j
        \end{dcases}
        \end{equation}
        Hence, 
        by applying \ref{thm:HellysTheorem:3}, 
        we have 
        \begin{align*}
        \abs{\ip{\tilde{x}, x_j^*}-\alpha_j} 
        & = \abs{\ip{\tilde{x}, \sum\limits_{i=1}^m \beta_ix_i^*}-\alpha_j}\\
        & = \abs{\pa{\sum_{i=1}^n \beta_i \ip{\tilde{x}, x_i^*}} + \pa{-1} \alpha_j}\\
        & = \abs{\sum\limits_{i=1}^n \beta_i\alpha_i} \\ 
        & \leq M \norm{\sum\limits_{i=1}^n \beta_i x_i^*}\\
        & = M \norm{\sum\limits_{i=1}^m \gamma_ix_i^* + (-1) x_j^*}\\
        & = 0
        \end{align*}
        Hence $\ip{\tilde{x}, x_j^*} = \alpha_j$. 
        Since $m < j$ was arbitrary, 
        $\ip{\tilde{x}, x_i^*} = \alpha_i$ for 
        $m < i \leq n$, $i \in \mathbb{Z}$. 
        \begin{equation*}
        \label{eq:HellysTheorem:2}
        \ip{x_0, x_i^*} = \alpha_i 
        \end{equation*}
        for $1 \leq i \leq n$
        and for any $x_0 \in \tilde{x}+K$. 
        Define $T:x+K \to \mathbb{R}$ by 
        $Tx=d(x, K)$. 
        Then $T \in \pa{x+K}^*$, so by 
        \ref{thm:HahnBanach:Extension1}
        there exists $x^* \in X^*$ satisfying
        $\norm{x^*}=1$, 
        $\ip{\tilde{x}, x^*} = d(\tilde{x}, K)$, 
        and $Kernel(S) = K \subset ker(x^*)$. 
        Since $\bigcap\limits_{i=1}^n Kernel(x_i^*) =Kernel(S) \subset Kernl(x^*)$, 
        by \ref{prop:FunctionalKernelIntersection}, 
        $x^* \in span(x_1^*, \cdots, x_n^*)$. 
        Hence we can find a representation 
        $x^* = \sum\limits_{i=1}^n \mu_i x_i^*$. 
        (The representation is not unique)
        Hence, 
        \begin{align*}
           d\pa{\tilde{x}, K} & = \ip{\tilde{x}, x^*} \\ 
           & = \sum\limits_{i=1}^n \mu_i \ip{\tilde{x}, x_i^*}\\
           & = \sum\limits_{i=1}^n \mu_i \alpha_i \\
           & \leq M \norm{\sum\limits_{i=1}^n \mu_i x_i^*} \\
           & = M \norm{x^*} \\
           & = M
        \end{align*}
        Let $\epsilon > 0$. 
        Then we can find $x_0 \in K$ such that $\norm{\tilde{x}-x_0} < M+\epsilon$, and 
        by \ref{eq:HellysTheorem:2}, 
        $\ip{\tilde{x}-x_0, x_i^*} = \alpha_i$ for $1 \leq i \leq n$. 
    \end{proof}
\end{thm}


